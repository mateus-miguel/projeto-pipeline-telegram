{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cab6aba",
   "metadata": {
    "papermill": {
     "duration": 0.004718,
     "end_time": "2023-10-18T12:35:42.967333",
     "exception": false,
     "start_time": "2023-10-18T12:35:42.962615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Projeto de Pipeline de Dados do Telegram\n",
    "\n",
    "Este projeto visa relacionar o mundo de chatbots em aplicativos como o Telegram com computação em nuvem. No caso, é realizada uma **etapa transacional** para capturar dados da API do Telegram de bots pelo AWS API Gateway, e a partir disto uma **etapa analítica** realizada em nuvem na AWS com três etapas: ingestão, ETL e apresentação. Por fim, no AWS Athena são feitas consultas para estimar quantidade de mensagens por dia no grupo do Telegram, quantidade de mensagens por hora e dia da semana, e média de tamanho das mensagens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcfc067",
   "metadata": {
    "papermill": {
     "duration": 0.003837,
     "end_time": "2023-10-18T12:35:42.978608",
     "exception": false,
     "start_time": "2023-10-18T12:35:42.974771",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Contexto\n",
    "\n",
    "Cada vez mais aplicativos e sites costumam contar com a presença de *chatbots*. Exemplos são Telegram, WhatsApp, Facebook Messenger, Discord e Microsoft Teams, para nomear alguns. Suas funções são amplas, como atendimento ao cliente, ajuda em compras, agendamentos, entretenimento. \n",
    "\n",
    "<img src='https://github.com/mateus-miguel/projeto-pipeline-telegram/blob/main/imagens/chatbot_img.jpg?raw=true'/>\n",
    "\n",
    "Nesse sentido, torna-se útil armazenar mensagens desses chats em base de dados para serem analisadas. Por exemplo, analisar quais mensagens ou dúvidas são mais frequentes, quais respostas são mais esperadas, qual a média de tamanho das mensagens, quais usuários costumam mandar mais mensagens, em quais grupos, e assim por diante.\n",
    "\n",
    "Em geral, as mensagens são fornecidas por API's para uso externo, e envolvem formatos de dados semi-estruturados como JSON. Para a seleção da parte útil desses dados para consultas de forma estruturada, é preciso realizar um *data wrangling* para colocá-los em formato tabular, além de outros processos para reduzir o custo de escaneamento. Isto pode ser feito por uma arquitetura em nuvem, como na AWS (Amazon Web Services), que permite realizar consultas por SQL ao fim do processo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87df2be1",
   "metadata": {
    "papermill": {
     "duration": 0.003635,
     "end_time": "2023-10-18T12:35:42.986349",
     "exception": false,
     "start_time": "2023-10-18T12:35:42.982714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Arquitetura\n",
    "\n",
    "O projeto se divide em duas partes como citado. A primeira é a **parte transacional**, que lida com o grupo do Telegram e a API de bots, já a segunda é a **parte analítica**, que contém as etapas de ingestão, ETL e apresentação.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mateus-miguel/projeto-pipeline-telegram/main/imagens/aws_telegram_pipeline_v2.png' width='700'/>\n",
    "\n",
    "A primeira etapa consiste na criação do *bot* do Telegram e sua adição no grupo específico. Nesta etapa, o *bot* vai coletar todas as mensagens mandadas pelos usuários ou si mesmo para a Telegram Bot API, que pode ser \n",
    "\n",
    "Na segunda etapa começamos pela **ingestão**, onde é criado um *webhook* pelo AWS API Gateway de forma a conectar com a Telegram Bot API. Desta forma, toda nova mensagem mandada no grupo é automaticamente mandada para o ambiente em nuvem da AWS. Então as mensagens passam por uma função AWS Lambda que armazena os arquivos em formato JSON original num bucket AWS S3 cru (*raw*).\n",
    "\n",
    "Em seguida, temos a etapa de **ETL** que cria um gatilho no AWS Event Bridge às 00:00 BRT para manipular os dados do *datalake* de arquivos JSON. Aqui, é feito um *data wrangling* para extrair apenas as chaves úteis dos arquivos JSON, e armazenar o conjunto de todos as mensagens num só arquivo diário no formato Apache Parquet. O armazenamento é feito de forma **particionada** por dia, assim como feito na etapa de ingestão, mas agora num Bucket S3 enriquecido (*enriched*). Este formato é **orientado a colunas**, então reduz bastante o custo de escaneamento dos dados.\n",
    "\n",
    "Por fim, é feita a etapa de **apresentação**, na qual outro gatilho do AWS Event Bridge é disparado às 02:00 BRT de forma a reparar a tabela de dados *telegram* com novas partições do *datalake* enriquecido com os arquivos Apache Parquet. Com isto, são feitas consultas SQL de interesse no AWS Athena na tabela particionada, analisando métricas sobre o grupo, usuários e mensagens enviadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3a4f0f",
   "metadata": {
    "papermill": {
     "duration": 0.003834,
     "end_time": "2023-10-18T12:35:42.994199",
     "exception": false,
     "start_time": "2023-10-18T12:35:42.990365",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Análise Exploratória de Dados\n",
    "\n",
    "A Telegram Bot API permite a comunicação de informações de mensagens, usuários e *bots* de um grupo com códigos externos. Existem alguns métodos que podem ser usados no Python para recuperar essas informações em formato JSON. Um deles é o método **getMe** que retorna informações sobre o *bot* do grupo. Já o método **getUpdates** retorna todas as informações com listas das mensagens mandadas em um grupo, disponíveis por até 24 horas. Para o acesso, é necessário um API token relacionado ao *bot*. Isso envolve usar linhas de código do tipo:\n",
    "\n",
    "```\n",
    "import json\n",
    "from getpass import getpass\n",
    "\n",
    "token = getpass()\n",
    "base_url = f'https://api.telegram.org/bot{token}'\n",
    " \n",
    "response = requests.get(url=f'{base_url}/getMe')\n",
    "```\n",
    "\n",
    "onde *token* é o API token do *bot* obtido do Telegram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b5b62a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T12:35:43.004316Z",
     "iopub.status.busy": "2023-10-18T12:35:43.003922Z",
     "iopub.status.idle": "2023-10-18T12:35:43.024716Z",
     "shell.execute_reply": "2023-10-18T12:35:43.023495Z"
    },
    "papermill": {
     "duration": 0.029051,
     "end_time": "2023-10-18T12:35:43.027305",
     "exception": false,
     "start_time": "2023-10-18T12:35:42.998254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ok\": true,\n",
      "  \"result\": {\n",
      "    \"id\": 6654125182,\n",
      "    \"is_bot\": true,\n",
      "    \"first_name\": \"m42_ebac_bot\",\n",
      "    \"username\": \"m42_pipeline_bot\",\n",
      "    \"can_join_groups\": false,\n",
      "    \"can_read_all_group_messages\": false,\n",
      "    \"supports_inline_queries\": false\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "with open('/kaggle/input/data-getupdates/getMe.json', mode='r', encoding='utf8') as f:\n",
    "    message = json.loads(f.read())\n",
    "    print(json.dumps(\n",
    "        message, indent=2\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16562d15",
   "metadata": {
    "papermill": {
     "duration": 0.004402,
     "end_time": "2023-10-18T12:35:43.036202",
     "exception": false,
     "start_time": "2023-10-18T12:35:43.031800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "No método **getUpdates** temos informações mais relevantes das mensagens. Algumas chaves são **obrigatórias**, por exemplo first_name, is_bot, date, text. Mas, outras chaves são **opcionais** como last_name e username. Com um arquivo JSON de resposta do **getUpdates** armazenado como exemplo, podemos lê-lo para ver seu formato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c1b7d0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T12:35:43.046414Z",
     "iopub.status.busy": "2023-10-18T12:35:43.045933Z",
     "iopub.status.idle": "2023-10-18T12:35:43.057090Z",
     "shell.execute_reply": "2023-10-18T12:35:43.055731Z"
    },
    "papermill": {
     "duration": 0.019239,
     "end_time": "2023-10-18T12:35:43.059579",
     "exception": false,
     "start_time": "2023-10-18T12:35:43.040340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ok\": true,\n",
      "  \"result\": [\n",
      "    {\n",
      "      \"update_id\": 187921657,\n",
      "      \"message\": {\n",
      "        \"message_id\": 3,\n",
      "        \"from\": {\n",
      "          \"id\": 479372888,\n",
      "          \"is_bot\": false,\n",
      "          \"first_name\": \"Mateus\",\n",
      "          \"last_name\": \"Miguel\",\n",
      "          \"username\": \"mateusmmiguel\"\n",
      "        },\n",
      "        \"chat\": {\n",
      "          \"id\": -4055988830,\n",
      "          \"title\": \"M42 Ebac Group\",\n",
      "          \"type\": \"group\",\n",
      "          \"all_members_are_administrators\": true\n",
      "        },\n",
      "        \"date\": 1697323306,\n",
      "        \"text\": \"Ol\\u00e1, mundo!\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"update_id\": 187921658,\n",
      "      \"message\": {\n",
      "        \"message_id\": 4,\n",
      "        \"from\": {\n",
      "          \"id\": 479372888,\n",
      "          \"is_bot\": false,\n",
      "          \"first_name\": \"Mateus\",\n",
      "          \"last_name\": \"Miguel\",\n",
      "          \"username\": \"mateusmmiguel\"\n",
      "        },\n",
      "        \"chat\": {\n",
      "          \"id\": -4055988830,\n",
      "          \"title\": \"M42 Ebac Group\",\n",
      "          \"type\": \"group\",\n",
      "          \"all_members_are_administrators\": true\n",
      "        },\n",
      "        \"date\": 1697325209,\n",
      "        \"text\": \"Tudo bem com voc\\u00ea?\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open('/kaggle/input/data-getupdates/telegram.json', mode='r', encoding='utf8') as f:\n",
    "    message = json.loads(f.read())\n",
    "    print(json.dumps(\n",
    "        message, indent=2\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba1f72e",
   "metadata": {
    "papermill": {
     "duration": 0.004057,
     "end_time": "2023-10-18T12:35:43.068404",
     "exception": false,
     "start_time": "2023-10-18T12:35:43.064347",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Como o arquivo JSON é semi-estruturado, a ideia é fazer um *data wrangling* para extrair apenas as informações obrigatórias das mensagens e armazená-las em um formato estruturado, tabular. Com isso, vai ser possível usar o AWS Athena para consultas SQL padronizadas, sobre a tabela 'telegram'. A função *parse_data* é capaz de fazer isto, basicamente percorrendo as chaves e listas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186a88eb",
   "metadata": {
    "papermill": {
     "duration": 0.003894,
     "end_time": "2023-10-18T12:35:43.076529",
     "exception": false,
     "start_time": "2023-10-18T12:35:43.072635",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "```\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "def parse_data(data: dict) -> dict:\n",
    "    # Função que faz o data wrangling dos arquivos JSON\n",
    "    parsed_data = dict()\n",
    "    \n",
    "    for key, value in data.items():\n",
    "        if key == 'from':\n",
    "            for k, v in data[key].items():\n",
    "                if k in ['id', 'is_bot', 'first_name']:\n",
    "                    parsed_data[f'user_{k}'] = [v]\n",
    "        \n",
    "        elif key == 'chat':\n",
    "            for k, v in data[key].items():\n",
    "                if k in ['id', 'type']:\n",
    "                    parsed_data[f'chat_{k}'] = [v]\n",
    "                    \n",
    "        elif key in ['message_id', 'text']:\n",
    "            parsed_data[key] = [value]\n",
    "            \n",
    "        elif key == 'date':\n",
    "            tzinfo = timezone(offset=timedelta(hours=-3))\n",
    "            parsed_data[key] = [value]\n",
    "            parsed_data['timestamp'] = [datetime.fromtimestamp(value, tzinfo).strftime('%Y-%m-%d %H:%M:%S')]\n",
    "            \n",
    "    if not 'text' in parsed_data.keys():\n",
    "        parsed_data['text'] = ['']\n",
    "        \n",
    "    return parsed_data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebdff16",
   "metadata": {
    "papermill": {
     "duration": 0.00381,
     "end_time": "2023-10-18T12:35:43.084555",
     "exception": false,
     "start_time": "2023-10-18T12:35:43.080745",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.1 - Ingestão\n",
    "\n",
    "### 3.1.1 - AWS API Gateway\n",
    "\n",
    "O serviço em nuvem AWS API Gateway permite a criação de API's de forma escalável. No contexto deste projeto, é preciso conectar a Telegram Bot API com uma REST API criada neste serviço. No momento da implantação, é retornada a variável *aws_api_gateway_url* que contém a URL da REST API. Para conectá-la ao Telegram, é preciso usar o método **setWebhook**, já possuindo a variável *token* do *bot* do Telegram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e7b03b",
   "metadata": {
    "papermill": {
     "duration": 0.003992,
     "end_time": "2023-10-18T12:35:43.092870",
     "exception": false,
     "start_time": "2023-10-18T12:35:43.088878",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "```\n",
    "import json\n",
    "import requests\n",
    "from getpass import getpass\n",
    "\n",
    "token = getpass()\n",
    "base_url = f'https://api.telegram.org/bot{token}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f43ed4",
   "metadata": {
    "papermill": {
     "duration": 0.003757,
     "end_time": "2023-10-18T12:35:43.100849",
     "exception": false,
     "start_time": "2023-10-18T12:35:43.097092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Então é usado o método \n",
    "\n",
    "`response = requests.get(url=f'{base_url}/setWebhook?url={aws_api_gateway_url}'`\n",
    "\n",
    "Que cria um *webhook* entre as API's e retorna um JSON do formato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83871d2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T12:35:43.111055Z",
     "iopub.status.busy": "2023-10-18T12:35:43.110516Z",
     "iopub.status.idle": "2023-10-18T12:35:43.121471Z",
     "shell.execute_reply": "2023-10-18T12:35:43.119730Z"
    },
    "papermill": {
     "duration": 0.019309,
     "end_time": "2023-10-18T12:35:43.124157",
     "exception": false,
     "start_time": "2023-10-18T12:35:43.104848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ok\": true,\n",
      "  \"result\": true,\n",
      "  \"description\": \"Webhook was set\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open('../input/data-getupdates/setwebhook.json', mode='r', encoding='utf8') as f:\n",
    "    message = json.loads(f.read())\n",
    "    print(json.dumps(\n",
    "        message, indent=2\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7988603d",
   "metadata": {
    "papermill": {
     "duration": 0.00486,
     "end_time": "2023-10-18T12:35:43.133734",
     "exception": false,
     "start_time": "2023-10-18T12:35:43.128874",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Por fim, o método **getWebhookInfo** permite obter informações sobre o *webhook* criado entre as duas API's. Em específico, retorna informações sobre status, URL e endereço de IP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81f06c53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T12:35:43.144859Z",
     "iopub.status.busy": "2023-10-18T12:35:43.144363Z",
     "iopub.status.idle": "2023-10-18T12:35:43.155714Z",
     "shell.execute_reply": "2023-10-18T12:35:43.154604Z"
    },
    "papermill": {
     "duration": 0.021336,
     "end_time": "2023-10-18T12:35:43.159711",
     "exception": false,
     "start_time": "2023-10-18T12:35:43.138375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ok\": true,\n",
      "  \"result\": {\n",
      "    \"url\": \"https://biewk7y6uh.execute-api.sa-east-1.amazonaws.com/dev\",\n",
      "    \"has_custom_certificate\": false,\n",
      "    \"pending_update_count\": 0,\n",
      "    \"max_connections\": 40,\n",
      "    \"ip_address\": \"52.67.105.244\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open('../input/data-getupdates/webhookInfo.json', mode='r', encoding='utf8') as f:\n",
    "    message = json.loads(f.read())\n",
    "    print(json.dumps(\n",
    "        message, indent=2\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc36b8b3",
   "metadata": {
    "papermill": {
     "duration": 0.004247,
     "end_time": "2023-10-18T12:35:43.168989",
     "exception": false,
     "start_time": "2023-10-18T12:35:43.164742",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.1.2 - AWS Lambda Raw\n",
    "\n",
    "Com o *webhook* criado, agora os arquivos JSON das mensagens do Telegram são armazenados em um AWS S3 bucket armazenado na variável de ambiente AWS_S3_BUCKET cujo valor é o caminho do bucket respectivo. Outra variável de ambiente é TELEGRAM_CHAT_ID que armazena o id do chat da API, para conferir se os dados estão sendo fornecidos pelo chat correto.\n",
    "\n",
    "Além disso, a biblioteca *datetime* permite colocar os horários no fuso horário local (BRT) pelo uso de *timezone* e *timedelta*, com diferença de -03:00 horas em relação ao UTC. Isso deixa a manipulação de datas adequada aos horários das mensagens como visualizado no grupo do Telegram.\n",
    "\n",
    "Este processo é feito pela seguinte função AWS Lambda:\n",
    "\n",
    "```\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import boto3\n",
    "\n",
    "def lambda_handler(event: dict, context: dict) -> dict:\n",
    "  \"\"\"\n",
    "  Recebe uma mensagem do Telegram via AWS API Gateway, verifica\n",
    "  se seu conteúdo foi produzido em determinado grupo e escreve\n",
    "  em seu formato original JSON, em um bucket AWS S3\n",
    "  \"\"\"\n",
    "\n",
    "  # variáveis de ambiente\n",
    "\n",
    "  BUCKET = os.environ['AWS_S3_BUCKET']\n",
    "  TELEGRAM_CHAT_ID = int(os.environ['TELEGRAM_CHAT_ID'])\n",
    "\n",
    "  # variáveis lógicas\n",
    "\n",
    "  tzinfo = timezone(offset=timedelta(hours=-3))\n",
    "  date = datetime.now(tzinfo).strftime('%Y-%m-%d')\n",
    "  timestamp = datetime.now(tzinfo).strftime('%Y%m%d%H%M%S%f')\n",
    "\n",
    "  filename = f'{timestamp}.json'\n",
    "\n",
    "  # código principal\n",
    "\n",
    "  client = boto3.client('s3')\n",
    "\n",
    "  try:\n",
    "    message = json.loads(event['body'])\n",
    "    # message = event\n",
    "    chat_id = message['message']['chat']['id']\n",
    "\n",
    "    if chat_id == TELEGRAM_CHAT_ID:\n",
    "      with open(f'/tmp/{filename}', mode='w', encoding='utf8') as f:\n",
    "        json.dump(message, f)\n",
    "      client.upload_file(f'/tmp/{filename}', BUCKET, f'telegram/context_date={date}/{filename}')\n",
    "      \n",
    "  except Exception as exc:\n",
    "    logging.error(msg=exc)\n",
    "    return dict(statusCode='500')\n",
    "    \n",
    "  else:\n",
    "    return dict(statusCode='200')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc4ae46",
   "metadata": {
    "papermill": {
     "duration": 0.00442,
     "end_time": "2023-10-18T12:35:43.177929",
     "exception": false,
     "start_time": "2023-10-18T12:35:43.173509",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.2 - ETL\n",
    "\n",
    "Nesta etapa, é programado um gatilho às 00:00 BRT (fuso horário local) no AWS EventBridge por meio da expressão cron (0 3 \\* \\* ? \\*). O seu objetivo é disparar uma função AWS Lambda que realiza o *data wrangling* dos arquivos JSON do dia anterior inteiro para apenas um arquivo Apache Parquet. Portanto, envolve duas variáveis de ambiente AWS_S3_RAW e AWS_S3_ENRICHED dos dois buckets envolvidos, o primeiro com os arquivos JSON armazenados por partições diárias e o segundo com arquivos PARQUET particionados também diariamente. Por meio do AWS IAM são garantidas permissões de acesso como AmazonS3FullAccess para a interação da função com os buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09ed902",
   "metadata": {
    "papermill": {
     "duration": 0.004215,
     "end_time": "2023-10-18T12:35:43.186465",
     "exception": false,
     "start_time": "2023-10-18T12:35:43.182250",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "```\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "import boto3\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def lambda_handler(event: dict, context: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Função que puxa os arquivos do dia anterior do raw bucket S3\n",
    "    e realiza um data wrangling de todos os arquivos JSON para persistir\n",
    "    como tabela do formato .parquet dentro do enriched bucket S3\n",
    "    \"\"\"\n",
    "    \n",
    "    # variáveis de ambiente\n",
    "    \n",
    "    RAW_BUCKET = os.environ['AWS_S3_RAW']\n",
    "    ENRICHED_BUCKET = os.environ['AWS_S3_ENRICHED']\n",
    "    \n",
    "    # variáveis lógicas\n",
    "    \n",
    "    tzinfo = timezone(offset=timedelta(hours=-3))\n",
    "    date = (datetime.now(tzinfo) - timedelta(days=1)).strftime('%Y-%m-%d') # dia anterior, com offset de timedelta(days=1)\n",
    "    timestamp = datetime.now(tzinfo).strftime('%Y%m%d%H%M%S%f')\n",
    "    \n",
    "    # código principal\n",
    "    \n",
    "    table = None\n",
    "    client = boto3.client('s3')\n",
    "    \n",
    "    try:\n",
    "        # listando arquivos JSON do Raw Bucket pela pasta do dia anterior\n",
    "        response = client.list_objects_v2(Bucket=RAW_BUCKET, Prefix=f'telegram/context_date={date}')\n",
    "        \n",
    "        for content in response['Contents']:\n",
    "            key = content['Key']\n",
    "            arquivo = key.split('/')[-1]\n",
    "            client.download_file(RAW_BUCKET, key, f'/tmp/{arquivo}')\n",
    "            \n",
    "            with open(f'/tmp/{arquivo}', mode='r', encoding='utf8') as f:\n",
    "                data = json.load(f)\n",
    "                data = data['message']\n",
    "\n",
    "            # É feito data wrangling para formato tabular, e então usado o PyArrow para criar uma table .parquet\n",
    "            parsed_data = parse_data(data=data) # data wrangling\n",
    "            iter_table = pa.Table.from_pydict(mapping=parsed_data)\n",
    "            \n",
    "            if table:\n",
    "                table = pa.concat_tables([table, iter_table]) # concatenação dos arquivos JSON diários em forma tabular\n",
    "            else:\n",
    "                table = iter_table\n",
    "                iter_table = None\n",
    "                \n",
    "        pq.write_table(table=table, where=f'/tmp/{timestamp}.parquet')\n",
    "        client.upload_file(f'/tmp/{timestamp}.parquet', ENRICHED_BUCKET, f'context_date={date}/{timestamp}.parquet')\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as exc:\n",
    "        logging.error(msg=exc)\n",
    "        return False \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c0dc30",
   "metadata": {
    "papermill": {
     "duration": 0.004011,
     "end_time": "2023-10-18T12:35:43.194775",
     "exception": false,
     "start_time": "2023-10-18T12:35:43.190764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Na função AWS Lambda acima, além das permissões e variáveis de ambiente, é preciso instalar uma *Layer* que permita o uso de bibliotecas como PyArrow. Isto porque as funções AWS Lambda normalmente têm acesso a poucos pacotes, em geral nativos do Python. Porém, para criar os arquivos Apache Parquet é preciso o uso de pyarrow e pyarrow.parquet. Através do repositório GitHub [AWS SDK for pandas (awswrangler)](http://https://github.com/aws/aws-sdk-pandas/releases) podemos baixar o arquivo ZIP relacionado com a versão do Python escolhida para a função AWS Lambda, no caso do projeto Python 3.8. Como este arquivo é grande da ordem de megabytes (MB), é antes adicionado a um bucket do AWS S3 para então ser criada uma nova *Layer* apontando para ele."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f55f8b",
   "metadata": {
    "papermill": {
     "duration": 0.004145,
     "end_time": "2023-10-18T12:35:43.203260",
     "exception": false,
     "start_time": "2023-10-18T12:35:43.199115",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.3 - Apresentação\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.342828,
   "end_time": "2023-10-18T12:35:43.731076",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-18T12:35:38.388248",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
