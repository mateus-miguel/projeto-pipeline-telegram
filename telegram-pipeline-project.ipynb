{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67896a02",
   "metadata": {
    "papermill": {
     "duration": 0.00447,
     "end_time": "2023-10-18T12:56:36.460898",
     "exception": false,
     "start_time": "2023-10-18T12:56:36.456428",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Projeto de Pipeline de Dados do Telegram\n",
    "\n",
    "Este projeto visa relacionar o mundo de chatbots em aplicativos como o Telegram com computação em nuvem. No caso, é realizada uma **etapa transacional** para capturar dados da API do Telegram de bots pelo AWS API Gateway, e a partir disto uma **etapa analítica** realizada em nuvem na AWS com três etapas: ingestão, ETL e apresentação. Por fim, no AWS Athena são feitas consultas para estimar quantidade de mensagens por dia no grupo do Telegram, quantidade de mensagens por hora e dia da semana, e média de tamanho das mensagens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafa4a35",
   "metadata": {
    "papermill": {
     "duration": 0.003628,
     "end_time": "2023-10-18T12:56:36.470200",
     "exception": false,
     "start_time": "2023-10-18T12:56:36.466572",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Contexto\n",
    "\n",
    "Cada vez mais aplicativos e sites costumam contar com a presença de *chatbots*. Exemplos são Telegram, WhatsApp, Facebook Messenger, Discord e Microsoft Teams, para nomear alguns. Suas funções são amplas, como atendimento ao cliente, ajuda em compras, agendamentos, entretenimento. \n",
    "\n",
    "<img src='https://github.com/mateus-miguel/projeto-pipeline-telegram/blob/main/imagens/chatbot_img.jpg?raw=true'/>\n",
    "\n",
    "Nesse sentido, torna-se útil armazenar mensagens desses chats em base de dados para serem analisadas. Por exemplo, analisar quais mensagens ou dúvidas são mais frequentes, quais respostas são mais esperadas, qual a média de tamanho das mensagens, quais usuários costumam mandar mais mensagens, em quais grupos, e assim por diante.\n",
    "\n",
    "Em geral, as mensagens são fornecidas por API's para uso externo, e envolvem formatos de dados semi-estruturados como JSON. Para a seleção da parte útil desses dados para consultas de forma estruturada, é preciso realizar um *data wrangling* para colocá-los em formato tabular, além de outros processos para reduzir o custo de escaneamento. Isto pode ser feito por uma arquitetura em nuvem, como na AWS (Amazon Web Services), que permite realizar consultas por SQL ao fim do processo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d2cb68",
   "metadata": {
    "papermill": {
     "duration": 0.003614,
     "end_time": "2023-10-18T12:56:36.477621",
     "exception": false,
     "start_time": "2023-10-18T12:56:36.474007",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Arquitetura\n",
    "\n",
    "O projeto se divide em duas partes como citado. A primeira é a **parte transacional**, que lida com o grupo do Telegram e a API de bots, já a segunda é a **parte analítica**, que contém as etapas de ingestão, ETL e apresentação.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mateus-miguel/projeto-pipeline-telegram/main/imagens/aws_telegram_pipeline_v2.png' width='700'/>\n",
    "\n",
    "A primeira etapa consiste na criação do *bot* do Telegram e sua adição no grupo específico. Nesta etapa, o *bot* vai coletar todas as mensagens mandadas pelos usuários ou si mesmo para a Telegram Bot API, que pode ser \n",
    "\n",
    "Na segunda etapa começamos pela **ingestão**, onde é criado um *webhook* pelo AWS API Gateway de forma a conectar com a Telegram Bot API. Desta forma, toda nova mensagem mandada no grupo é automaticamente mandada para o ambiente em nuvem da AWS. Então as mensagens passam por uma função AWS Lambda que armazena os arquivos em formato JSON original num bucket AWS S3 cru (*raw*).\n",
    "\n",
    "Em seguida, temos a etapa de **ETL** que cria um gatilho no AWS Event Bridge às 00:00 BRT para manipular os dados do *datalake* de arquivos JSON. Aqui, é feito um *data wrangling* para extrair apenas as chaves úteis dos arquivos JSON, e armazenar o conjunto de todos as mensagens num só arquivo diário no formato Apache Parquet. O armazenamento é feito de forma **particionada** por dia, assim como feito na etapa de ingestão, mas agora num Bucket S3 enriquecido (*enriched*). Este formato é **orientado a colunas**, então reduz bastante o custo de escaneamento dos dados.\n",
    "\n",
    "Por fim, é feita a etapa de **apresentação**, na qual outro gatilho do AWS Event Bridge é disparado às 02:00 BRT de forma a reparar a tabela de dados *telegram* com novas partições do *datalake* enriquecido com os arquivos Apache Parquet. Com isto, são feitas consultas SQL de interesse no AWS Athena na tabela particionada, analisando métricas sobre o grupo, usuários e mensagens enviadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f830a2dc",
   "metadata": {
    "papermill": {
     "duration": 0.003539,
     "end_time": "2023-10-18T12:56:36.484902",
     "exception": false,
     "start_time": "2023-10-18T12:56:36.481363",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Análise Exploratória de Dados\n",
    "\n",
    "A Telegram Bot API permite a comunicação de informações de mensagens, usuários e *bots* de um grupo com códigos externos. Existem alguns métodos que podem ser usados no Python para recuperar essas informações em formato JSON. Um deles é o método **getMe** que retorna informações sobre o *bot* do grupo. Já o método **getUpdates** retorna todas as informações com listas das mensagens mandadas em um grupo, disponíveis por até 24 horas. Para o acesso, é necessário um API token relacionado ao *bot*. Isso envolve usar linhas de código do tipo:\n",
    "\n",
    "```\n",
    "import json\n",
    "from getpass import getpass\n",
    "\n",
    "token = getpass()\n",
    "base_url = f'https://api.telegram.org/bot{token}'\n",
    " \n",
    "response = requests.get(url=f'{base_url}/getMe')\n",
    "```\n",
    "\n",
    "onde *token* é o API token do *bot* obtido do Telegram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "807607cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T12:56:36.494591Z",
     "iopub.status.busy": "2023-10-18T12:56:36.493928Z",
     "iopub.status.idle": "2023-10-18T12:56:36.506574Z",
     "shell.execute_reply": "2023-10-18T12:56:36.505492Z"
    },
    "papermill": {
     "duration": 0.01965,
     "end_time": "2023-10-18T12:56:36.508377",
     "exception": false,
     "start_time": "2023-10-18T12:56:36.488727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ok\": true,\n",
      "  \"result\": {\n",
      "    \"id\": 6654125182,\n",
      "    \"is_bot\": true,\n",
      "    \"first_name\": \"m42_ebac_bot\",\n",
      "    \"username\": \"m42_pipeline_bot\",\n",
      "    \"can_join_groups\": false,\n",
      "    \"can_read_all_group_messages\": false,\n",
      "    \"supports_inline_queries\": false\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "with open('/kaggle/input/data-getupdates/getMe.json', mode='r', encoding='utf8') as f:\n",
    "    message = json.loads(f.read())\n",
    "    print(json.dumps(\n",
    "        message, indent=2\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cad35",
   "metadata": {
    "papermill": {
     "duration": 0.003775,
     "end_time": "2023-10-18T12:56:36.516244",
     "exception": false,
     "start_time": "2023-10-18T12:56:36.512469",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "No método **getUpdates** temos informações mais relevantes das mensagens. Algumas chaves são **obrigatórias**, por exemplo first_name, is_bot, date, text. Mas, outras chaves são **opcionais** como last_name e username. Com um arquivo JSON de resposta do **getUpdates** armazenado como exemplo, podemos lê-lo para ver seu formato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7960731e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T12:56:36.525707Z",
     "iopub.status.busy": "2023-10-18T12:56:36.525090Z",
     "iopub.status.idle": "2023-10-18T12:56:36.536954Z",
     "shell.execute_reply": "2023-10-18T12:56:36.535543Z"
    },
    "papermill": {
     "duration": 0.018526,
     "end_time": "2023-10-18T12:56:36.538727",
     "exception": false,
     "start_time": "2023-10-18T12:56:36.520201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ok\": true,\n",
      "  \"result\": [\n",
      "    {\n",
      "      \"update_id\": 187921657,\n",
      "      \"message\": {\n",
      "        \"message_id\": 3,\n",
      "        \"from\": {\n",
      "          \"id\": 479372888,\n",
      "          \"is_bot\": false,\n",
      "          \"first_name\": \"Mateus\",\n",
      "          \"last_name\": \"Miguel\",\n",
      "          \"username\": \"mateusmmiguel\"\n",
      "        },\n",
      "        \"chat\": {\n",
      "          \"id\": -4055988830,\n",
      "          \"title\": \"M42 Ebac Group\",\n",
      "          \"type\": \"group\",\n",
      "          \"all_members_are_administrators\": true\n",
      "        },\n",
      "        \"date\": 1697323306,\n",
      "        \"text\": \"Ol\\u00e1, mundo!\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"update_id\": 187921658,\n",
      "      \"message\": {\n",
      "        \"message_id\": 4,\n",
      "        \"from\": {\n",
      "          \"id\": 479372888,\n",
      "          \"is_bot\": false,\n",
      "          \"first_name\": \"Mateus\",\n",
      "          \"last_name\": \"Miguel\",\n",
      "          \"username\": \"mateusmmiguel\"\n",
      "        },\n",
      "        \"chat\": {\n",
      "          \"id\": -4055988830,\n",
      "          \"title\": \"M42 Ebac Group\",\n",
      "          \"type\": \"group\",\n",
      "          \"all_members_are_administrators\": true\n",
      "        },\n",
      "        \"date\": 1697325209,\n",
      "        \"text\": \"Tudo bem com voc\\u00ea?\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open('/kaggle/input/data-getupdates/telegram.json', mode='r', encoding='utf8') as f:\n",
    "    message = json.loads(f.read())\n",
    "    print(json.dumps(\n",
    "        message, indent=2\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae1973",
   "metadata": {
    "papermill": {
     "duration": 0.003936,
     "end_time": "2023-10-18T12:56:36.546929",
     "exception": false,
     "start_time": "2023-10-18T12:56:36.542993",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Como o arquivo JSON é semi-estruturado, a ideia é fazer um *data wrangling* para extrair apenas as informações obrigatórias das mensagens e armazená-las em um formato estruturado, tabular. Com isso, vai ser possível usar o AWS Athena para consultas SQL padronizadas, sobre a tabela 'telegram'. A função *parse_data* é capaz de fazer isto, basicamente percorrendo as chaves e listas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fc3d7d",
   "metadata": {
    "papermill": {
     "duration": 0.00389,
     "end_time": "2023-10-18T12:56:36.554983",
     "exception": false,
     "start_time": "2023-10-18T12:56:36.551093",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "```\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "def parse_data(data: dict) -> dict:\n",
    "    # Função que faz o data wrangling dos arquivos JSON\n",
    "    parsed_data = dict()\n",
    "    \n",
    "    for key, value in data.items():\n",
    "        if key == 'from':\n",
    "            for k, v in data[key].items():\n",
    "                if k in ['id', 'is_bot', 'first_name']:\n",
    "                    parsed_data[f'user_{k}'] = [v]\n",
    "        \n",
    "        elif key == 'chat':\n",
    "            for k, v in data[key].items():\n",
    "                if k in ['id', 'type']:\n",
    "                    parsed_data[f'chat_{k}'] = [v]\n",
    "                    \n",
    "        elif key in ['message_id', 'text']:\n",
    "            parsed_data[key] = [value]\n",
    "            \n",
    "        elif key == 'date':\n",
    "            tzinfo = timezone(offset=timedelta(hours=-3))\n",
    "            parsed_data[key] = [value]\n",
    "            parsed_data['timestamp'] = [datetime.fromtimestamp(value, tzinfo).strftime('%Y-%m-%d %H:%M:%S')]\n",
    "            \n",
    "    if not 'text' in parsed_data.keys():\n",
    "        parsed_data['text'] = ['']\n",
    "        \n",
    "    return parsed_data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669b8f71",
   "metadata": {
    "papermill": {
     "duration": 0.003818,
     "end_time": "2023-10-18T12:56:36.562868",
     "exception": false,
     "start_time": "2023-10-18T12:56:36.559050",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.1 - Ingestão\n",
    "\n",
    "### 3.1.1 - AWS API Gateway\n",
    "\n",
    "O serviço em nuvem AWS API Gateway permite a criação de API's de forma escalável. No contexto deste projeto, é preciso conectar a Telegram Bot API com uma REST API criada neste serviço. No momento da implantação, é retornada a variável *aws_api_gateway_url* que contém a URL da REST API. Para conectá-la ao Telegram, é preciso usar o método **setWebhook**, já possuindo a variável *token* do *bot* do Telegram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc0d2ca",
   "metadata": {
    "papermill": {
     "duration": 0.003813,
     "end_time": "2023-10-18T12:56:36.570773",
     "exception": false,
     "start_time": "2023-10-18T12:56:36.566960",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "```\n",
    "import json\n",
    "import requests\n",
    "from getpass import getpass\n",
    "\n",
    "token = getpass()\n",
    "base_url = f'https://api.telegram.org/bot{token}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0355f34e",
   "metadata": {
    "papermill": {
     "duration": 0.003825,
     "end_time": "2023-10-18T12:56:36.578638",
     "exception": false,
     "start_time": "2023-10-18T12:56:36.574813",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Então é usado o método \n",
    "\n",
    "`response = requests.get(url=f'{base_url}/setWebhook?url={aws_api_gateway_url}'`\n",
    "\n",
    "Que cria um *webhook* entre as API's e retorna um JSON do formato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59ea1026",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T12:56:36.588745Z",
     "iopub.status.busy": "2023-10-18T12:56:36.587722Z",
     "iopub.status.idle": "2023-10-18T12:56:36.596642Z",
     "shell.execute_reply": "2023-10-18T12:56:36.595607Z"
    },
    "papermill": {
     "duration": 0.016407,
     "end_time": "2023-10-18T12:56:36.599102",
     "exception": false,
     "start_time": "2023-10-18T12:56:36.582695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ok\": true,\n",
      "  \"result\": true,\n",
      "  \"description\": \"Webhook was set\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open('../input/data-getupdates/setwebhook.json', mode='r', encoding='utf8') as f:\n",
    "    message = json.loads(f.read())\n",
    "    print(json.dumps(\n",
    "        message, indent=2\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620e8cae",
   "metadata": {
    "papermill": {
     "duration": 0.003931,
     "end_time": "2023-10-18T12:56:36.607755",
     "exception": false,
     "start_time": "2023-10-18T12:56:36.603824",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Por fim, o método **getWebhookInfo** permite obter informações sobre o *webhook* criado entre as duas API's. Em específico, retorna informações sobre status, URL e endereço de IP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d72e8ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T12:56:36.617380Z",
     "iopub.status.busy": "2023-10-18T12:56:36.616929Z",
     "iopub.status.idle": "2023-10-18T12:56:36.624129Z",
     "shell.execute_reply": "2023-10-18T12:56:36.623104Z"
    },
    "papermill": {
     "duration": 0.014112,
     "end_time": "2023-10-18T12:56:36.625915",
     "exception": false,
     "start_time": "2023-10-18T12:56:36.611803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ok\": true,\n",
      "  \"result\": {\n",
      "    \"url\": \"https://biewk7y6uh.execute-api.sa-east-1.amazonaws.com/dev\",\n",
      "    \"has_custom_certificate\": false,\n",
      "    \"pending_update_count\": 0,\n",
      "    \"max_connections\": 40,\n",
      "    \"ip_address\": \"52.67.105.244\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open('../input/data-getupdates/webhookInfo.json', mode='r', encoding='utf8') as f:\n",
    "    message = json.loads(f.read())\n",
    "    print(json.dumps(\n",
    "        message, indent=2\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64c4cd3",
   "metadata": {
    "papermill": {
     "duration": 0.00382,
     "end_time": "2023-10-18T12:56:36.633954",
     "exception": false,
     "start_time": "2023-10-18T12:56:36.630134",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.1.2 - AWS Lambda Raw\n",
    "\n",
    "Com o *webhook* criado, agora os arquivos JSON das mensagens do Telegram são armazenados em um AWS S3 bucket armazenado na variável de ambiente AWS_S3_BUCKET cujo valor é o caminho do bucket respectivo. Outra variável de ambiente é TELEGRAM_CHAT_ID que armazena o id do chat da API, para conferir se os dados estão sendo fornecidos pelo chat correto.\n",
    "\n",
    "Além disso, a biblioteca *datetime* permite colocar os horários no fuso horário local (BRT) pelo uso de *timezone* e *timedelta*, com diferença de -03:00 horas em relação ao UTC. Isso deixa a manipulação de datas adequada aos horários das mensagens como visualizado no grupo do Telegram.\n",
    "\n",
    "Este processo é feito pela seguinte função AWS Lambda:\n",
    "\n",
    "```\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import boto3\n",
    "\n",
    "def lambda_handler(event: dict, context: dict) -> dict:\n",
    "  \"\"\"\n",
    "  Recebe uma mensagem do Telegram via AWS API Gateway, verifica\n",
    "  se seu conteúdo foi produzido em determinado grupo e escreve\n",
    "  em seu formato original JSON, em um bucket AWS S3\n",
    "  \"\"\"\n",
    "\n",
    "  # variáveis de ambiente\n",
    "\n",
    "  BUCKET = os.environ['AWS_S3_BUCKET']\n",
    "  TELEGRAM_CHAT_ID = int(os.environ['TELEGRAM_CHAT_ID'])\n",
    "\n",
    "  # variáveis lógicas\n",
    "\n",
    "  tzinfo = timezone(offset=timedelta(hours=-3))\n",
    "  date = datetime.now(tzinfo).strftime('%Y-%m-%d')\n",
    "  timestamp = datetime.now(tzinfo).strftime('%Y%m%d%H%M%S%f')\n",
    "\n",
    "  filename = f'{timestamp}.json'\n",
    "\n",
    "  # código principal\n",
    "\n",
    "  client = boto3.client('s3')\n",
    "\n",
    "  try:\n",
    "    message = json.loads(event['body'])\n",
    "    # message = event\n",
    "    chat_id = message['message']['chat']['id']\n",
    "\n",
    "    if chat_id == TELEGRAM_CHAT_ID:\n",
    "      with open(f'/tmp/{filename}', mode='w', encoding='utf8') as f:\n",
    "        json.dump(message, f)\n",
    "      client.upload_file(f'/tmp/{filename}', BUCKET, f'telegram/context_date={date}/{filename}')\n",
    "      \n",
    "  except Exception as exc:\n",
    "    logging.error(msg=exc)\n",
    "    return dict(statusCode='500')\n",
    "    \n",
    "  else:\n",
    "    return dict(statusCode='200')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dd40da",
   "metadata": {
    "papermill": {
     "duration": 0.003856,
     "end_time": "2023-10-18T12:56:36.641828",
     "exception": false,
     "start_time": "2023-10-18T12:56:36.637972",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.2 - ETL\n",
    "\n",
    "Nesta etapa, é programado um gatilho às 00:00 BRT (fuso horário local) no AWS EventBridge por meio da expressão cron (0 3 \\* \\* ? \\*). O seu objetivo é disparar uma função AWS Lambda que realiza o *data wrangling* dos arquivos JSON do dia anterior inteiro para apenas um arquivo Apache Parquet. Portanto, envolve duas variáveis de ambiente AWS_S3_RAW e AWS_S3_ENRICHED dos dois buckets envolvidos, o primeiro com os arquivos JSON armazenados por partições diárias e o segundo com arquivos PARQUET particionados também diariamente. Por meio do AWS IAM são garantidas permissões de acesso como AmazonS3FullAccess para a interação da função com os buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88488c",
   "metadata": {
    "papermill": {
     "duration": 0.003853,
     "end_time": "2023-10-18T12:56:36.649681",
     "exception": false,
     "start_time": "2023-10-18T12:56:36.645828",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "```\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "import boto3\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def lambda_handler(event: dict, context: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Função que puxa os arquivos do dia anterior do raw bucket S3\n",
    "    e realiza um data wrangling de todos os arquivos JSON para persistir\n",
    "    como tabela do formato .parquet dentro do enriched bucket S3\n",
    "    \"\"\"\n",
    "    \n",
    "    # variáveis de ambiente\n",
    "    \n",
    "    RAW_BUCKET = os.environ['AWS_S3_RAW']\n",
    "    ENRICHED_BUCKET = os.environ['AWS_S3_ENRICHED']\n",
    "    \n",
    "    # variáveis lógicas\n",
    "    \n",
    "    tzinfo = timezone(offset=timedelta(hours=-3))\n",
    "    date = (datetime.now(tzinfo) - timedelta(days=1)).strftime('%Y-%m-%d') # dia anterior, com offset de timedelta(days=1)\n",
    "    timestamp = datetime.now(tzinfo).strftime('%Y%m%d%H%M%S%f')\n",
    "    \n",
    "    # código principal\n",
    "    \n",
    "    table = None\n",
    "    client = boto3.client('s3')\n",
    "    \n",
    "    try:\n",
    "        # listando arquivos JSON do Raw Bucket pela pasta do dia anterior\n",
    "        response = client.list_objects_v2(Bucket=RAW_BUCKET, Prefix=f'telegram/context_date={date}')\n",
    "        \n",
    "        for content in response['Contents']:\n",
    "            key = content['Key']\n",
    "            arquivo = key.split('/')[-1]\n",
    "            client.download_file(RAW_BUCKET, key, f'/tmp/{arquivo}')\n",
    "            \n",
    "            with open(f'/tmp/{arquivo}', mode='r', encoding='utf8') as f:\n",
    "                data = json.load(f)\n",
    "                data = data['message']\n",
    "\n",
    "            # É feito data wrangling para formato tabular, e então usado o PyArrow para criar uma table .parquet\n",
    "            parsed_data = parse_data(data=data) # data wrangling\n",
    "            iter_table = pa.Table.from_pydict(mapping=parsed_data)\n",
    "            \n",
    "            if table:\n",
    "                table = pa.concat_tables([table, iter_table]) # concatenação dos arquivos JSON diários em forma tabular\n",
    "            else:\n",
    "                table = iter_table\n",
    "                iter_table = None\n",
    "                \n",
    "        pq.write_table(table=table, where=f'/tmp/{timestamp}.parquet')\n",
    "        client.upload_file(f'/tmp/{timestamp}.parquet', ENRICHED_BUCKET, f'context_date={date}/{timestamp}.parquet')\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as exc:\n",
    "        logging.error(msg=exc)\n",
    "        return False \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd7690b",
   "metadata": {
    "papermill": {
     "duration": 0.003948,
     "end_time": "2023-10-18T12:56:36.657898",
     "exception": false,
     "start_time": "2023-10-18T12:56:36.653950",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Na função AWS Lambda acima, além das permissões e variáveis de ambiente, é preciso instalar uma *Layer* que permita o uso de bibliotecas como PyArrow. Isto porque as funções AWS Lambda normalmente têm acesso a poucos pacotes, em geral nativos do Python. Porém, para criar os arquivos Apache Parquet é preciso o uso de pyarrow e pyarrow.parquet. Através do repositório GitHub [AWS SDK for pandas (awswrangler)](http://https://github.com/aws/aws-sdk-pandas/releases) podemos baixar o arquivo ZIP relacionado com a versão do Python escolhida para a função AWS Lambda, no caso do projeto Python 3.8. Como este arquivo é grande da ordem de megabytes (MB), é antes adicionado a um bucket do AWS S3 para então ser criada uma nova *Layer* apontando para ele.\n",
    "\n",
    "Na etapa de criação da tabela Apache Parquet, é usada a função `pa.Table.from_pydict(mapping=parsed_data)` onde `parsed_data` é o dicionário Python retornado após o *data wrangling* extraindo e formatando o nome das colunas de interesse. É feito um laço de repetição sobre todos os arquivos acessados pela função `S3.Client.list_objects_v2()`, de forma a ir concatenando as tabelas PyArrow por `pa.concat_tables([table, iter_table])` até reunir todos os JSON de um mesmo dia. \n",
    "\n",
    "Através do pacote PyArrow podemos visualizar como fica um arquivo Apache Parquet das partições após executada a função AWS Lambda com o *data wrangling* adequado. O exemplo abaixo contém as variáveis `last_name` e `username` que são opcionais, e não são usadas no projeto final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3acbf0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T12:56:36.667917Z",
     "iopub.status.busy": "2023-10-18T12:56:36.667137Z",
     "iopub.status.idle": "2023-10-18T12:56:37.183369Z",
     "shell.execute_reply": "2023-10-18T12:56:37.182469Z"
    },
    "papermill": {
     "duration": 0.523711,
     "end_time": "2023-10-18T12:56:37.185678",
     "exception": false,
     "start_time": "2023-10-18T12:56:36.661967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "table = pq.read_table('../input/data-getupdates/20231016163208693232.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad5716e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T12:56:37.195818Z",
     "iopub.status.busy": "2023-10-18T12:56:37.195362Z",
     "iopub.status.idle": "2023-10-18T12:56:37.201417Z",
     "shell.execute_reply": "2023-10-18T12:56:37.200837Z"
    },
    "papermill": {
     "duration": 0.012922,
     "end_time": "2023-10-18T12:56:37.203099",
     "exception": false,
     "start_time": "2023-10-18T12:56:37.190177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "message_id: int64\n",
       "user_id: int64\n",
       "user_is_bot: bool\n",
       "user_first_name: string\n",
       "user_last_name: string\n",
       "user_username: string\n",
       "chat_id: int64\n",
       "chat_type: string\n",
       "date: int64\n",
       "timestamp: string\n",
       "text: string\n",
       "----\n",
       "message_id: [[9,10,11]]\n",
       "user_id: [[479372888,479372888,479372888]]\n",
       "user_is_bot: [[false,false,false]]\n",
       "user_first_name: [[\"Mateus\",\"Mateus\",\"Mateus\"]]\n",
       "user_last_name: [[\"Miguel\",\"Miguel\",\"Miguel\"]]\n",
       "user_username: [[\"mateusmmiguel\",\"mateusmmiguel\",\"mateusmmiguel\"]]\n",
       "chat_id: [[-4055988830,-4055988830,-4055988830]]\n",
       "chat_type: [[\"group\",\"group\",\"group\"]]\n",
       "date: [[1697472788,1697473103,1697473104]]\n",
       "timestamp: [[\"2023-10-16 13:13:08\",\"2023-10-16 13:18:23\",\"2023-10-16 13:18:24\"]]\n",
       "..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c1479d",
   "metadata": {
    "papermill": {
     "duration": 0.004033,
     "end_time": "2023-10-18T12:56:37.211565",
     "exception": false,
     "start_time": "2023-10-18T12:56:37.207532",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.3 - Apresentação\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.012912,
   "end_time": "2023-10-18T12:56:37.533320",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-18T12:56:33.520408",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
