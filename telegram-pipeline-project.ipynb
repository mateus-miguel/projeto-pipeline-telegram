{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f557c46b",
   "metadata": {
    "papermill": {
     "duration": 0.006344,
     "end_time": "2023-10-18T20:20:25.835367",
     "exception": false,
     "start_time": "2023-10-18T20:20:25.829023",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Projeto de Pipeline de Dados do Telegram\n",
    "\n",
    "Este projeto visa relacionar o mundo de chatbots em aplicativos como o Telegram com computação em nuvem. No caso, é realizada uma **etapa transacional** para capturar dados da API do Telegram de bots pelo AWS API Gateway, e a partir disto uma **etapa analítica** realizada em nuvem na AWS com três etapas: ingestão, ETL e apresentação. Por fim, no AWS Athena são feitas consultas para estimar quantidade de mensagens por dia no grupo do Telegram, quantidade de mensagens por hora e dia da semana, e média de tamanho das mensagens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eac279",
   "metadata": {
    "papermill": {
     "duration": 0.006338,
     "end_time": "2023-10-18T20:20:25.848528",
     "exception": false,
     "start_time": "2023-10-18T20:20:25.842190",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Contexto\n",
    "\n",
    "Cada vez mais aplicativos e sites costumam contar com a presença de *chatbots*. Exemplos são Telegram, WhatsApp, Facebook Messenger, Discord e Microsoft Teams, para nomear alguns. Suas funções são amplas, como atendimento ao cliente, ajuda em compras, agendamentos, entretenimento. \n",
    "\n",
    "<img src='https://github.com/mateus-miguel/projeto-pipeline-telegram/blob/main/imagens/chatbot_img.jpg?raw=true'/>\n",
    "\n",
    "Nesse sentido, torna-se útil armazenar mensagens desses chats em base de dados para serem analisadas. Por exemplo, analisar quais mensagens ou dúvidas são mais frequentes, quais respostas são mais esperadas, qual a média de tamanho das mensagens, quais usuários costumam mandar mais mensagens, em quais grupos, e assim por diante.\n",
    "\n",
    "Em geral, as mensagens são fornecidas por API's para uso externo, e envolvem formatos de dados semi-estruturados como JSON. Para a seleção da parte útil desses dados para consultas de forma estruturada, é preciso realizar um *data wrangling* para colocá-los em formato tabular, além de outros processos para reduzir o custo de escaneamento. Isto pode ser feito por uma arquitetura em nuvem, como na AWS (Amazon Web Services), que permite realizar consultas por SQL ao fim do processo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59b56fb",
   "metadata": {
    "papermill": {
     "duration": 0.005389,
     "end_time": "2023-10-18T20:20:25.860851",
     "exception": false,
     "start_time": "2023-10-18T20:20:25.855462",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Arquitetura\n",
    "\n",
    "O projeto se divide em duas partes como citado. A primeira é a **parte transacional**, que lida com o grupo do Telegram e a API de bots, já a segunda é a **parte analítica**, que contém as etapas de ingestão, ETL e apresentação.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/mateus-miguel/projeto-pipeline-telegram/main/imagens/aws_telegram_pipeline_v2.png' width='700'/>\n",
    "\n",
    "A primeira etapa consiste na criação do *bot* do Telegram e sua adição no grupo específico. Nesta etapa, o *bot* vai coletar todas as mensagens mandadas pelos usuários ou si mesmo para a Telegram Bot API, que pode ser \n",
    "\n",
    "Na segunda etapa começamos pela **ingestão**, onde é criado um *webhook* pelo AWS API Gateway de forma a conectar com a Telegram Bot API. Desta forma, toda nova mensagem mandada no grupo é automaticamente mandada para o ambiente em nuvem da AWS. Então as mensagens passam por uma função AWS Lambda que armazena os arquivos em formato JSON original num bucket AWS S3 cru (*raw*).\n",
    "\n",
    "Em seguida, temos a etapa de **ETL** que cria um gatilho no AWS Event Bridge às 00:00 BRT para manipular os dados do *datalake* de arquivos JSON. Aqui, é feito um *data wrangling* para extrair apenas as chaves úteis dos arquivos JSON, e armazenar o conjunto de todos as mensagens num só arquivo diário no formato Apache Parquet. O armazenamento é feito de forma **particionada** por dia, assim como feito na etapa de ingestão, mas agora num Bucket S3 enriquecido (*enriched*). Este formato é **orientado a colunas**, então reduz bastante o custo de escaneamento dos dados.\n",
    "\n",
    "Por fim, é feita a etapa de **apresentação**, na qual outro gatilho do AWS Event Bridge é disparado às 02:00 BRT de forma a reparar a tabela de dados *telegram* com novas partições do *datalake* enriquecido com os arquivos Apache Parquet. Com isto, são feitas consultas SQL de interesse no AWS Athena na tabela particionada, analisando métricas sobre o grupo, usuários e mensagens enviadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f76dcd1",
   "metadata": {
    "papermill": {
     "duration": 0.005332,
     "end_time": "2023-10-18T20:20:25.872617",
     "exception": false,
     "start_time": "2023-10-18T20:20:25.867285",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Análise Exploratória de Dados\n",
    "\n",
    "A Telegram Bot API permite a comunicação de informações de mensagens, usuários e *bots* de um grupo com códigos externos. Existem alguns métodos que podem ser usados no Python para recuperar essas informações em formato JSON. Um deles é o método **getMe** que retorna informações sobre o *bot* do grupo. Já o método **getUpdates** retorna todas as informações com listas das mensagens mandadas em um grupo, disponíveis por até 24 horas. Para o acesso, é necessário um API token relacionado ao *bot*. Isso envolve usar linhas de código do tipo:\n",
    "\n",
    "```\n",
    "import json\n",
    "from getpass import getpass\n",
    "\n",
    "token = getpass()\n",
    "base_url = f'https://api.telegram.org/bot{token}'\n",
    " \n",
    "response = requests.get(url=f'{base_url}/getMe')\n",
    "```\n",
    "\n",
    "onde *token* é o API token do *bot* obtido do Telegram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8664f451",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T20:20:25.887014Z",
     "iopub.status.busy": "2023-10-18T20:20:25.886087Z",
     "iopub.status.idle": "2023-10-18T20:20:25.903290Z",
     "shell.execute_reply": "2023-10-18T20:20:25.902492Z"
    },
    "papermill": {
     "duration": 0.02707,
     "end_time": "2023-10-18T20:20:25.905419",
     "exception": false,
     "start_time": "2023-10-18T20:20:25.878349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ok\": true,\n",
      "  \"result\": {\n",
      "    \"id\": 6654125182,\n",
      "    \"is_bot\": true,\n",
      "    \"first_name\": \"m42_ebac_bot\",\n",
      "    \"username\": \"m42_pipeline_bot\",\n",
      "    \"can_join_groups\": false,\n",
      "    \"can_read_all_group_messages\": false,\n",
      "    \"supports_inline_queries\": false\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "with open('/kaggle/input/data-getupdates/getMe.json', mode='r', encoding='utf8') as f:\n",
    "    message = json.loads(f.read())\n",
    "    print(json.dumps(\n",
    "        message, indent=2\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e321e1",
   "metadata": {
    "papermill": {
     "duration": 0.005657,
     "end_time": "2023-10-18T20:20:25.917269",
     "exception": false,
     "start_time": "2023-10-18T20:20:25.911612",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "No método **getUpdates** temos informações mais relevantes das mensagens. Algumas chaves são **obrigatórias**, por exemplo first_name, is_bot, date, text. Mas, outras chaves são **opcionais** como last_name e username. Com um arquivo JSON de resposta do **getUpdates** armazenado como exemplo, podemos lê-lo para ver seu formato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3c78393",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T20:20:25.930356Z",
     "iopub.status.busy": "2023-10-18T20:20:25.929960Z",
     "iopub.status.idle": "2023-10-18T20:20:25.940786Z",
     "shell.execute_reply": "2023-10-18T20:20:25.938806Z"
    },
    "papermill": {
     "duration": 0.020469,
     "end_time": "2023-10-18T20:20:25.943390",
     "exception": false,
     "start_time": "2023-10-18T20:20:25.922921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ok\": true,\n",
      "  \"result\": [\n",
      "    {\n",
      "      \"update_id\": 187921657,\n",
      "      \"message\": {\n",
      "        \"message_id\": 3,\n",
      "        \"from\": {\n",
      "          \"id\": 479372888,\n",
      "          \"is_bot\": false,\n",
      "          \"first_name\": \"Mateus\",\n",
      "          \"last_name\": \"Miguel\",\n",
      "          \"username\": \"mateusmmiguel\"\n",
      "        },\n",
      "        \"chat\": {\n",
      "          \"id\": -4055988830,\n",
      "          \"title\": \"M42 Ebac Group\",\n",
      "          \"type\": \"group\",\n",
      "          \"all_members_are_administrators\": true\n",
      "        },\n",
      "        \"date\": 1697323306,\n",
      "        \"text\": \"Ol\\u00e1, mundo!\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"update_id\": 187921658,\n",
      "      \"message\": {\n",
      "        \"message_id\": 4,\n",
      "        \"from\": {\n",
      "          \"id\": 479372888,\n",
      "          \"is_bot\": false,\n",
      "          \"first_name\": \"Mateus\",\n",
      "          \"last_name\": \"Miguel\",\n",
      "          \"username\": \"mateusmmiguel\"\n",
      "        },\n",
      "        \"chat\": {\n",
      "          \"id\": -4055988830,\n",
      "          \"title\": \"M42 Ebac Group\",\n",
      "          \"type\": \"group\",\n",
      "          \"all_members_are_administrators\": true\n",
      "        },\n",
      "        \"date\": 1697325209,\n",
      "        \"text\": \"Tudo bem com voc\\u00ea?\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open('/kaggle/input/data-getupdates/telegram.json', mode='r', encoding='utf8') as f:\n",
    "    message = json.loads(f.read())\n",
    "    print(json.dumps(\n",
    "        message, indent=2\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e9a873",
   "metadata": {
    "papermill": {
     "duration": 0.006618,
     "end_time": "2023-10-18T20:20:25.957539",
     "exception": false,
     "start_time": "2023-10-18T20:20:25.950921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Como o arquivo JSON é semi-estruturado, a ideia é fazer um *data wrangling* para extrair apenas as informações obrigatórias das mensagens e armazená-las em um formato estruturado, tabular. Com isso, vai ser possível usar o AWS Athena para consultas SQL padronizadas, sobre a tabela 'telegram'. A função *parse_data* é capaz de fazer isto, basicamente percorrendo as chaves e listas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5a7535",
   "metadata": {
    "papermill": {
     "duration": 0.006331,
     "end_time": "2023-10-18T20:20:25.970415",
     "exception": false,
     "start_time": "2023-10-18T20:20:25.964084",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "```\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "def parse_data(data: dict) -> dict:\n",
    "    # Função que faz o data wrangling dos arquivos JSON\n",
    "    parsed_data = dict()\n",
    "    \n",
    "    for key, value in data.items():\n",
    "        if key == 'from':\n",
    "            for k, v in data[key].items():\n",
    "                if k in ['id', 'is_bot', 'first_name']:\n",
    "                    parsed_data[f'user_{k}'] = [v]\n",
    "        \n",
    "        elif key == 'chat':\n",
    "            for k, v in data[key].items():\n",
    "                if k in ['id', 'type']:\n",
    "                    parsed_data[f'chat_{k}'] = [v]\n",
    "                    \n",
    "        elif key in ['message_id', 'text']:\n",
    "            parsed_data[key] = [value]\n",
    "            \n",
    "        elif key == 'date':\n",
    "            tzinfo = timezone(offset=timedelta(hours=-3))\n",
    "            parsed_data[key] = [value]\n",
    "            parsed_data['timestamp'] = [datetime.fromtimestamp(value, tzinfo).strftime('%Y-%m-%d %H:%M:%S')]\n",
    "            \n",
    "    if not 'text' in parsed_data.keys():\n",
    "        parsed_data['text'] = ['']\n",
    "        \n",
    "    return parsed_data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f68813e",
   "metadata": {
    "papermill": {
     "duration": 0.006847,
     "end_time": "2023-10-18T20:20:25.983512",
     "exception": false,
     "start_time": "2023-10-18T20:20:25.976665",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.1 - Ingestão\n",
    "\n",
    "### 3.1.1 - AWS API Gateway\n",
    "\n",
    "O serviço em nuvem AWS API Gateway permite a criação de API's de forma escalável. No contexto deste projeto, é preciso conectar a Telegram Bot API com uma REST API criada neste serviço. No momento da implantação, é retornada a variável *aws_api_gateway_url* que contém a URL da REST API. Para conectá-la ao Telegram, é preciso usar o método **setWebhook**, já possuindo a variável *token* do *bot* do Telegram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae117083",
   "metadata": {
    "papermill": {
     "duration": 0.006333,
     "end_time": "2023-10-18T20:20:25.995977",
     "exception": false,
     "start_time": "2023-10-18T20:20:25.989644",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "```\n",
    "import json\n",
    "import requests\n",
    "from getpass import getpass\n",
    "\n",
    "token = getpass()\n",
    "base_url = f'https://api.telegram.org/bot{token}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efedb19",
   "metadata": {
    "papermill": {
     "duration": 0.005647,
     "end_time": "2023-10-18T20:20:26.007461",
     "exception": false,
     "start_time": "2023-10-18T20:20:26.001814",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Então é usado o método \n",
    "\n",
    "`response = requests.get(url=f'{base_url}/setWebhook?url={aws_api_gateway_url}'`\n",
    "\n",
    "Que cria um *webhook* entre as API's e retorna um JSON do formato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba862204",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T20:20:26.021831Z",
     "iopub.status.busy": "2023-10-18T20:20:26.021166Z",
     "iopub.status.idle": "2023-10-18T20:20:26.030668Z",
     "shell.execute_reply": "2023-10-18T20:20:26.029523Z"
    },
    "papermill": {
     "duration": 0.01973,
     "end_time": "2023-10-18T20:20:26.033090",
     "exception": false,
     "start_time": "2023-10-18T20:20:26.013360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ok\": true,\n",
      "  \"result\": true,\n",
      "  \"description\": \"Webhook was set\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open('../input/data-getupdates/setwebhook.json', mode='r', encoding='utf8') as f:\n",
    "    message = json.loads(f.read())\n",
    "    print(json.dumps(\n",
    "        message, indent=2\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4269f5a",
   "metadata": {
    "papermill": {
     "duration": 0.005556,
     "end_time": "2023-10-18T20:20:26.044663",
     "exception": false,
     "start_time": "2023-10-18T20:20:26.039107",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Por fim, o método **getWebhookInfo** permite obter informações sobre o *webhook* criado entre as duas API's. Em específico, retorna informações sobre status, URL e endereço de IP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "056ef188",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T20:20:26.060897Z",
     "iopub.status.busy": "2023-10-18T20:20:26.060479Z",
     "iopub.status.idle": "2023-10-18T20:20:26.069882Z",
     "shell.execute_reply": "2023-10-18T20:20:26.068552Z"
    },
    "papermill": {
     "duration": 0.019971,
     "end_time": "2023-10-18T20:20:26.072122",
     "exception": false,
     "start_time": "2023-10-18T20:20:26.052151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ok\": true,\n",
      "  \"result\": {\n",
      "    \"url\": \"https://biewk7y6uh.execute-api.sa-east-1.amazonaws.com/dev\",\n",
      "    \"has_custom_certificate\": false,\n",
      "    \"pending_update_count\": 0,\n",
      "    \"max_connections\": 40,\n",
      "    \"ip_address\": \"52.67.105.244\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open('../input/data-getupdates/webhookInfo.json', mode='r', encoding='utf8') as f:\n",
    "    message = json.loads(f.read())\n",
    "    print(json.dumps(\n",
    "        message, indent=2\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e28875b",
   "metadata": {
    "papermill": {
     "duration": 0.006523,
     "end_time": "2023-10-18T20:20:26.084641",
     "exception": false,
     "start_time": "2023-10-18T20:20:26.078118",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.1.2 - AWS Lambda Raw\n",
    "\n",
    "Com o *webhook* criado, agora os arquivos JSON das mensagens do Telegram são armazenados em um AWS S3 bucket armazenado na variável de ambiente AWS_S3_BUCKET cujo valor é o caminho do bucket respectivo. Outra variável de ambiente é TELEGRAM_CHAT_ID que armazena o id do chat da API, para conferir se os dados estão sendo fornecidos pelo chat correto.\n",
    "\n",
    "Além disso, a biblioteca *datetime* permite colocar os horários no fuso horário local (BRT) pelo uso de *timezone* e *timedelta*, com diferença de -03:00 horas em relação ao UTC. Isso deixa a manipulação de datas adequada aos horários das mensagens como visualizado no grupo do Telegram.\n",
    "\n",
    "Este processo é feito pela seguinte função AWS Lambda:\n",
    "\n",
    "```\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import boto3\n",
    "\n",
    "def lambda_handler(event: dict, context: dict) -> dict:\n",
    "  \"\"\"\n",
    "  Recebe uma mensagem do Telegram via AWS API Gateway, verifica\n",
    "  se seu conteúdo foi produzido em determinado grupo e escreve\n",
    "  em seu formato original JSON, em um bucket AWS S3\n",
    "  \"\"\"\n",
    "\n",
    "  # variáveis de ambiente\n",
    "\n",
    "  BUCKET = os.environ['AWS_S3_BUCKET']\n",
    "  TELEGRAM_CHAT_ID = int(os.environ['TELEGRAM_CHAT_ID'])\n",
    "\n",
    "  # variáveis lógicas\n",
    "\n",
    "  tzinfo = timezone(offset=timedelta(hours=-3))\n",
    "  date = datetime.now(tzinfo).strftime('%Y-%m-%d')\n",
    "  timestamp = datetime.now(tzinfo).strftime('%Y%m%d%H%M%S%f')\n",
    "\n",
    "  filename = f'{timestamp}.json'\n",
    "\n",
    "  # código principal\n",
    "\n",
    "  client = boto3.client('s3')\n",
    "\n",
    "  try:\n",
    "    message = json.loads(event['body'])\n",
    "    # message = event\n",
    "    chat_id = message['message']['chat']['id']\n",
    "\n",
    "    if chat_id == TELEGRAM_CHAT_ID:\n",
    "      with open(f'/tmp/{filename}', mode='w', encoding='utf8') as f:\n",
    "        json.dump(message, f)\n",
    "      client.upload_file(f'/tmp/{filename}', BUCKET, f'telegram/context_date={date}/{filename}')\n",
    "      \n",
    "  except Exception as exc:\n",
    "    logging.error(msg=exc)\n",
    "    return dict(statusCode='500')\n",
    "    \n",
    "  else:\n",
    "    return dict(statusCode='200')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4060d7b",
   "metadata": {
    "papermill": {
     "duration": 0.005903,
     "end_time": "2023-10-18T20:20:26.097678",
     "exception": false,
     "start_time": "2023-10-18T20:20:26.091775",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.2 - ETL\n",
    "\n",
    "Nesta etapa, é programado um gatilho às 00:00 BRT (fuso horário local) no AWS EventBridge por meio da expressão cron (0 3 \\* \\* ? \\*). O seu objetivo é disparar uma função AWS Lambda que realiza o *data wrangling* dos arquivos JSON do dia anterior inteiro para apenas um arquivo Apache Parquet. Portanto, envolve duas variáveis de ambiente AWS_S3_RAW e AWS_S3_ENRICHED dos dois buckets envolvidos, o primeiro com os arquivos JSON armazenados por partições diárias e o segundo com arquivos PARQUET particionados também diariamente. Por meio do AWS IAM são garantidas permissões de acesso como AmazonS3FullAccess para a interação da função com os buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c42e9d",
   "metadata": {
    "papermill": {
     "duration": 0.005999,
     "end_time": "2023-10-18T20:20:26.109657",
     "exception": false,
     "start_time": "2023-10-18T20:20:26.103658",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "```\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "import boto3\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def lambda_handler(event: dict, context: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Função que puxa os arquivos do dia anterior do raw bucket S3\n",
    "    e realiza um data wrangling de todos os arquivos JSON para persistir\n",
    "    como tabela do formato .parquet dentro do enriched bucket S3\n",
    "    \"\"\"\n",
    "    \n",
    "    # variáveis de ambiente\n",
    "    \n",
    "    RAW_BUCKET = os.environ['AWS_S3_RAW']\n",
    "    ENRICHED_BUCKET = os.environ['AWS_S3_ENRICHED']\n",
    "    \n",
    "    # variáveis lógicas\n",
    "    \n",
    "    tzinfo = timezone(offset=timedelta(hours=-3))\n",
    "    date = (datetime.now(tzinfo) - timedelta(days=1)).strftime('%Y-%m-%d') # dia anterior, com offset de timedelta(days=1)\n",
    "    timestamp = datetime.now(tzinfo).strftime('%Y%m%d%H%M%S%f')\n",
    "    \n",
    "    # código principal\n",
    "    \n",
    "    table = None\n",
    "    client = boto3.client('s3')\n",
    "    \n",
    "    try:\n",
    "        # listando arquivos JSON do Raw Bucket pela pasta do dia anterior\n",
    "        response = client.list_objects_v2(Bucket=RAW_BUCKET, Prefix=f'telegram/context_date={date}')\n",
    "        \n",
    "        for content in response['Contents']:\n",
    "            key = content['Key']\n",
    "            arquivo = key.split('/')[-1]\n",
    "            client.download_file(RAW_BUCKET, key, f'/tmp/{arquivo}')\n",
    "            \n",
    "            with open(f'/tmp/{arquivo}', mode='r', encoding='utf8') as f:\n",
    "                data = json.load(f)\n",
    "                data = data['message']\n",
    "\n",
    "            # É feito data wrangling para formato tabular, e então usado o PyArrow para criar uma table .parquet\n",
    "            parsed_data = parse_data(data=data) # data wrangling\n",
    "            iter_table = pa.Table.from_pydict(mapping=parsed_data)\n",
    "            \n",
    "            if table:\n",
    "                table = pa.concat_tables([table, iter_table]) # concatenação dos arquivos JSON diários em forma tabular\n",
    "            else:\n",
    "                table = iter_table\n",
    "                iter_table = None\n",
    "                \n",
    "        pq.write_table(table=table, where=f'/tmp/{timestamp}.parquet')\n",
    "        client.upload_file(f'/tmp/{timestamp}.parquet', ENRICHED_BUCKET, f'context_date={date}/{timestamp}.parquet')\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as exc:\n",
    "        logging.error(msg=exc)\n",
    "        return False \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec56e44",
   "metadata": {
    "papermill": {
     "duration": 0.005809,
     "end_time": "2023-10-18T20:20:26.122080",
     "exception": false,
     "start_time": "2023-10-18T20:20:26.116271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Na função AWS Lambda acima, além das permissões e variáveis de ambiente, é preciso instalar uma *Layer* que permita o uso de bibliotecas como PyArrow. Isto porque as funções AWS Lambda normalmente têm acesso a poucos pacotes, em geral nativos do Python. Porém, para criar os arquivos Apache Parquet é preciso o uso de pyarrow e pyarrow.parquet. Através do repositório GitHub [AWS SDK for pandas (awswrangler)](http://https://github.com/aws/aws-sdk-pandas/releases) podemos baixar o arquivo ZIP relacionado com a versão do Python escolhida para a função AWS Lambda, no caso do projeto Python 3.8. Como este arquivo é grande da ordem de megabytes (MB), é antes adicionado a um bucket do AWS S3 para então ser criada uma nova *Layer* apontando para ele.\n",
    "\n",
    "Na etapa de criação da tabela Apache Parquet, é usada a função `pa.Table.from_pydict(mapping=parsed_data)` onde `parsed_data` é o dicionário Python retornado após o *data wrangling* extraindo e formatando o nome das colunas de interesse na função `parse_data()` descrita anteriormente. É feito um laço de repetição sobre todos os arquivos acessados pela função `S3.Client.list_objects_v2()`, de forma a ir concatenando as tabelas PyArrow por `pa.concat_tables([table, iter_table])` até reunir todos os JSON de um mesmo dia. \n",
    "\n",
    "Através do pacote PyArrow podemos visualizar como fica um arquivo Apache Parquet das partições após executada a função AWS Lambda com o *data wrangling* adequado. O exemplo abaixo contém as variáveis `last_name` e `username` que são opcionais, e não são usadas no projeto final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7a99033",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T20:20:26.135284Z",
     "iopub.status.busy": "2023-10-18T20:20:26.134944Z",
     "iopub.status.idle": "2023-10-18T20:20:26.793378Z",
     "shell.execute_reply": "2023-10-18T20:20:26.792183Z"
    },
    "papermill": {
     "duration": 0.66867,
     "end_time": "2023-10-18T20:20:26.796545",
     "exception": false,
     "start_time": "2023-10-18T20:20:26.127875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "table = pq.read_table('../input/data-getupdates/20231016163208693232.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec49fac4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T20:20:26.813905Z",
     "iopub.status.busy": "2023-10-18T20:20:26.813014Z",
     "iopub.status.idle": "2023-10-18T20:20:26.825210Z",
     "shell.execute_reply": "2023-10-18T20:20:26.823920Z"
    },
    "papermill": {
     "duration": 0.023931,
     "end_time": "2023-10-18T20:20:26.827902",
     "exception": false,
     "start_time": "2023-10-18T20:20:26.803971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "message_id: int64\n",
       "user_id: int64\n",
       "user_is_bot: bool\n",
       "user_first_name: string\n",
       "user_last_name: string\n",
       "user_username: string\n",
       "chat_id: int64\n",
       "chat_type: string\n",
       "date: int64\n",
       "timestamp: string\n",
       "text: string\n",
       "----\n",
       "message_id: [[9,10,11]]\n",
       "user_id: [[479372888,479372888,479372888]]\n",
       "user_is_bot: [[false,false,false]]\n",
       "user_first_name: [[\"Mateus\",\"Mateus\",\"Mateus\"]]\n",
       "user_last_name: [[\"Miguel\",\"Miguel\",\"Miguel\"]]\n",
       "user_username: [[\"mateusmmiguel\",\"mateusmmiguel\",\"mateusmmiguel\"]]\n",
       "chat_id: [[-4055988830,-4055988830,-4055988830]]\n",
       "chat_type: [[\"group\",\"group\",\"group\"]]\n",
       "date: [[1697472788,1697473103,1697473104]]\n",
       "timestamp: [[\"2023-10-16 13:13:08\",\"2023-10-16 13:18:23\",\"2023-10-16 13:18:24\"]]\n",
       "..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eece431d",
   "metadata": {
    "papermill": {
     "duration": 0.006589,
     "end_time": "2023-10-18T20:20:26.841293",
     "exception": false,
     "start_time": "2023-10-18T20:20:26.834704",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.3 - Apresentação\n",
    "\n",
    "### 3.3.1 - AWS EventBridge e AWS Lambda\n",
    "\n",
    "Nesta última etapa usamos outra gatilho do AWS EventBridge, agora às 02:00 BRT, para ativar uma terceira função AWS Lambda. Esta função vai ser responsável por executar a *query* que atualiza as partições na tabela *telegram* que é criada no AWS Athena. A *query* inicial cria a tabela através do bucket AWS S3 com os dados Apache Parquet enriquecidos.\n",
    "\n",
    "```\n",
    "CREATE EXTERNAL TABLE `telegram`(\n",
    "    `message_id` bigint,\n",
    "    `user_id` bigint,\n",
    "    `user_is_bot` boolean,\n",
    "    `user_first_name` string,\n",
    "    `chat_id` bigint,\n",
    "    `chat_type` string,\n",
    "    `date` bigint,\n",
    "    `timestamp` string,\n",
    "    `text` string\n",
    ") PARTITIONED BY (`context_date` string)\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
    "STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n",
    "LOCATION 's3://<AWS_S3_ENRICHED>/'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899e0911",
   "metadata": {
    "papermill": {
     "duration": 0.00616,
     "end_time": "2023-10-18T20:20:26.854586",
     "exception": false,
     "start_time": "2023-10-18T20:20:26.848426",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "O comando SQL responsável por atualizar as partições diárias nesta tabela é o seguinte:\n",
    "\n",
    "`MSCK REPAIR TABLE telegram` \n",
    "\n",
    "A função AWS Lambda então cria um cliente `client.boto3('athena')` que vai chamar um método `Athena.Client.start_query_execution()` que, portanto, é executado automaticamente em nuvem pelo gatilho configurado.\n",
    "\n",
    "```\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def lambda_handler(event: dict, context: dict) -> dict:\n",
    "    # Função executada às 02:00 BRT por uma regra do AWS Event Bridge para reparar a tabela \"telegram\" com novas partições\n",
    "    \n",
    "    # variáveis de ambiente\n",
    "    ENRICHED_BUCKET = os.environ['AWS_S3_ENRICHED']\n",
    "    \n",
    "    # código principal\n",
    "    client = boto3.client('athena')\n",
    "    query = \"MSCK REPAIR TABLE telegram\"\n",
    "    \n",
    "    try:\n",
    "        client.start_query_execution(\n",
    "            QueryString=query,\n",
    "            ResultConfiguration={\n",
    "                'OutputLocation': f's3://{ENRICHED_BUCKET}/'\n",
    "            }\n",
    "        )\n",
    "    except ClientError as exc:\n",
    "        raise exc\n",
    "        \n",
    "    return dict(statusCode=200)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47009332",
   "metadata": {
    "papermill": {
     "duration": 0.005932,
     "end_time": "2023-10-18T20:20:26.867434",
     "exception": false,
     "start_time": "2023-10-18T20:20:26.861502",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.3.2 - AWS Athena\n",
    "\n",
    "Com a tabela construída, é possível analisar informações das mensagens do chat agora em formato estruturado. Uma consulta interessante é tentar obter a quantidade de mensagens mandadas no grupo por dia, pelo comando:\n",
    "\n",
    "```\n",
    "SELECT context_date, COUNT(context_date) AS \"message_amount\"\n",
    "FROM telegram\n",
    "GROUP BY context_date\n",
    "ORDER BY context_date DESC;\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b134746",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T20:20:26.882850Z",
     "iopub.status.busy": "2023-10-18T20:20:26.881930Z",
     "iopub.status.idle": "2023-10-18T20:20:26.919365Z",
     "shell.execute_reply": "2023-10-18T20:20:26.917855Z"
    },
    "papermill": {
     "duration": 0.048391,
     "end_time": "2023-10-18T20:20:26.922013",
     "exception": false,
     "start_time": "2023-10-18T20:20:26.873622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_date</th>\n",
       "      <th>message_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-10-17</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-10-16</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  context_date  message_amount\n",
       "0   2023-10-17               5\n",
       "1   2023-10-16              17"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../input/data-getupdates/daily_messages.csv', sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea50c114",
   "metadata": {
    "papermill": {
     "duration": 0.006591,
     "end_time": "2023-10-18T20:20:26.935827",
     "exception": false,
     "start_time": "2023-10-18T20:20:26.929236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Outra consulta pode agrupar a quantidade de mensagens mandada por dia, mas por usuário/user_id. Isto é feito pelo seguinte comando SQL:\n",
    "\n",
    "```\n",
    "SELECT user_id, user_first_name, context_date, COUNT(user_id) AS \"message_amount\"\n",
    "FROM telegram\n",
    "GROUP BY user_id, user_first_name, context_date\n",
    "ORDER BY context_date DESC;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b2e7796",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T20:20:26.951772Z",
     "iopub.status.busy": "2023-10-18T20:20:26.951074Z",
     "iopub.status.idle": "2023-10-18T20:20:26.972131Z",
     "shell.execute_reply": "2023-10-18T20:20:26.970457Z"
    },
    "papermill": {
     "duration": 0.031547,
     "end_time": "2023-10-18T20:20:26.974571",
     "exception": false,
     "start_time": "2023-10-18T20:20:26.943024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_first_name</th>\n",
       "      <th>context_date</th>\n",
       "      <th>message_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1118401103</td>\n",
       "      <td>Vinicius</td>\n",
       "      <td>2023-10-17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>479372888</td>\n",
       "      <td>Mateus</td>\n",
       "      <td>2023-10-17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6652518189</td>\n",
       "      <td>Adi</td>\n",
       "      <td>2023-10-17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>479372888</td>\n",
       "      <td>Mateus</td>\n",
       "      <td>2023-10-16</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6652518189</td>\n",
       "      <td>Adi</td>\n",
       "      <td>2023-10-16</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id user_first_name context_date  message_amount\n",
       "0  1118401103        Vinicius   2023-10-17               1\n",
       "1   479372888          Mateus   2023-10-17               3\n",
       "2  6652518189             Adi   2023-10-17               1\n",
       "3   479372888          Mateus   2023-10-16              13\n",
       "4  6652518189             Adi   2023-10-16               4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../input/data-getupdates/daily_messages_by_user.csv', sep=',')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec471b02",
   "metadata": {
    "papermill": {
     "duration": 0.00705,
     "end_time": "2023-10-18T20:20:26.988059",
     "exception": false,
     "start_time": "2023-10-18T20:20:26.981009",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Também é possível obter o tamanho médio das mensagens por usuário e por data de partição, com mais um comando SQL.\n",
    "\n",
    "```\n",
    "SELECT user_id, user_first_name, context_date, CAST(AVG(length(text)) AS int) AS \"avg_message_length\"\n",
    "FROM telegram\n",
    "GROUP BY user_id, user_first_name, context_date\n",
    "ORDER BY context_date DESC;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03d41fea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T20:20:27.004869Z",
     "iopub.status.busy": "2023-10-18T20:20:27.004373Z",
     "iopub.status.idle": "2023-10-18T20:20:27.021001Z",
     "shell.execute_reply": "2023-10-18T20:20:27.019903Z"
    },
    "papermill": {
     "duration": 0.028821,
     "end_time": "2023-10-18T20:20:27.023282",
     "exception": false,
     "start_time": "2023-10-18T20:20:26.994461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_first_name</th>\n",
       "      <th>context_date</th>\n",
       "      <th>avg_message_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>479372888</td>\n",
       "      <td>Mateus</td>\n",
       "      <td>2023-10-17</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6652518189</td>\n",
       "      <td>Adi</td>\n",
       "      <td>2023-10-17</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1118401103</td>\n",
       "      <td>Vinicius</td>\n",
       "      <td>2023-10-17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>479372888</td>\n",
       "      <td>Mateus</td>\n",
       "      <td>2023-10-16</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6652518189</td>\n",
       "      <td>Adi</td>\n",
       "      <td>2023-10-16</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id user_first_name context_date  avg_message_length\n",
       "0   479372888          Mateus   2023-10-17                  21\n",
       "1  6652518189             Adi   2023-10-17                  18\n",
       "2  1118401103        Vinicius   2023-10-17                   0\n",
       "3   479372888          Mateus   2023-10-16                  19\n",
       "4  6652518189             Adi   2023-10-16                  12"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../input/data-getupdates/avg_message_length.csv', sep=',')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcc8acb",
   "metadata": {
    "papermill": {
     "duration": 0.007869,
     "end_time": "2023-10-18T20:20:27.037812",
     "exception": false,
     "start_time": "2023-10-18T20:20:27.029943",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Por fim, podemos fazer uma consulta mais elaborada usando a cláusula `WITH`. Com ela, criamos primeiro a tabela `parsed_table` que faz o *casting* da coluna `timestamp` com tipo timestamp, adicionando esta nova coluna às originais. Com esta tabela é feita outra `hour_table` que cria as colunas `parsed_date_hour` e `parsed_date_weekday` extraindo a hora e dia da semana das mensagens. Por fim, obtém-se a quantidade de mensagens agrupada por hora e dia da semana (domingo = 0), independente dos usuários. Isto ajuda a saber em que dias e horários o grupo é mais ativo, por exemplo.\n",
    "\n",
    "```\n",
    "WITH \n",
    "parsed_table AS (\n",
    "    SELECT *, CAST(\"timestamp\" AS timestamp) AS parsed_date\n",
    "    FROM telegram\n",
    "),\n",
    "hour_table AS (\n",
    "    SELECT *,\n",
    "        EXTRACT(hour FROM parsed_date) AS parsed_date_hour,\n",
    "        EXTRACT(dow FROM parsed_date) AS parsed_date_weekday\n",
    "    FROM parsed_table\n",
    ")\n",
    "SELECT parsed_date_hour, parsed_date_weekday, COUNT(user_id) AS \"message_amount\"\n",
    "FROM hour_table\n",
    "GROUP BY parsed_date_hour, parsed_date_weekday\n",
    "ORDER BY message_amount;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78e3de3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-18T20:20:27.055840Z",
     "iopub.status.busy": "2023-10-18T20:20:27.055343Z",
     "iopub.status.idle": "2023-10-18T20:20:27.076275Z",
     "shell.execute_reply": "2023-10-18T20:20:27.075067Z"
    },
    "papermill": {
     "duration": 0.033214,
     "end_time": "2023-10-18T20:20:27.078644",
     "exception": false,
     "start_time": "2023-10-18T20:20:27.045430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parsed_date_hour</th>\n",
       "      <th>parsed_date_weekday</th>\n",
       "      <th>message_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   parsed_date_hour  parsed_date_weekday  message_amount\n",
       "0                12                    2               1\n",
       "1                 0                    2               1\n",
       "2                 2                    2               1\n",
       "3                11                    2               2\n",
       "4                23                    1               2\n",
       "5                13                    1               3\n",
       "6                22                    1              12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../input/data-getupdates/parsed_date_hour_weekday_message_amount.csv', sep=',')\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.569155,
   "end_time": "2023-10-18T20:20:27.709393",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-18T20:20:22.140238",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
